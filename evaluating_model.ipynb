{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "# The following is only for working with RDDs\n",
    "# from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "# Setup a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('data/new_subset_data/final_train_data.csv')\n",
    "testdata = pd.read_csv('data/new_subset_data/final_test_data.csv')\n",
    "\n",
    "traindata = traindata.iloc[:300000,:]\n",
    "testdata = testdata.iloc[:100000,:]\n",
    "\n",
    "sp_train = spark.createDataFrame(traindata)\n",
    "sp_test = spark.createDataFrame(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change column names to something Spark has hardcoded into their GridSearch feature for dataframes\n",
    "oldColumns = sp_train.schema.names\n",
    "newColumns = [\"user\", \"item\", \"rating\"]\n",
    "\n",
    "sp_train = reduce(lambda sp_train, idx: sp_train.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), sp_train)\n",
    "sp_train.printSchema()\n",
    "\n",
    "#Change column names to something Spark has hardcoded into their GridSearch feature for dataframes\n",
    "oldColumns = sp_test.schema.names\n",
    "newColumns = [\"user\", \"item\", \"rating\"]\n",
    "\n",
    "sp_test = reduce(lambda sp_test, idx: sp_test.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), sp_test)\n",
    "sp_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_test_smaller = sc.parallelize(sp_test.take(100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load entire dataset for use in crossval gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entiredata = pd.read_csv('data/new_subset_data/ratings_data.csv', sep='\\t')\n",
    "\n",
    "sp_entire = spark.createDataFrame(entiredata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+----+----+------+\n",
      "|user|item|rating|\n",
      "+----+----+------+\n",
      "|   4|   2|   4.0|\n",
      "|  23|   3|   4.0|\n",
      "|  28|   5|   4.0|\n",
      "|  31|   6|   4.0|\n",
      "|  67|  10|   6.0|\n",
      "|  69|  11|   4.0|\n",
      "|  78|  19|   4.0|\n",
      "|  86|  23|   4.0|\n",
      "|  89|  27|   4.0|\n",
      "| 111|  29|   4.0|\n",
      "| 136|  31|   4.0|\n",
      "| 142|  34|   4.0|\n",
      "| 145|  35|   4.0|\n",
      "| 159|  37|   4.0|\n",
      "| 161|  39|   4.0|\n",
      "| 166|  40|   4.0|\n",
      "| 178|  42|   4.0|\n",
      "| 208|  49|   4.0|\n",
      "| 216|  52|   4.0|\n",
      "| 272|  55|   4.0|\n",
      "+----+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change column names to something Spark has hardcoded into their GridSearch feature for dataframes\n",
    "oldColumns = sp_entire.schema.names\n",
    "newColumns = [\"user\", \"item\", \"rating\"]\n",
    "\n",
    "sp_entire = reduce(lambda sp_entire, idx: sp_entire.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), sp_entire)\n",
    "sp_entire.printSchema()\n",
    "sp_entire.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ALS models and fit recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define models using different ranks\n",
    "model0 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=1,\n",
    "    maxIter=15\n",
    "    ) \n",
    "\n",
    "model1 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=5,\n",
    "    maxIter=15\n",
    "    ) \n",
    "\n",
    "model2 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=7,\n",
    "    maxIter=15\n",
    "    )\n",
    "\n",
    "model3 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=10,\n",
    "    maxIter=15\n",
    "    )\n",
    "\n",
    "model4 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=15,\n",
    "    maxIter=15\n",
    "    )\n",
    "\n",
    "model5 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=20,\n",
    "    maxIter=15\n",
    "    )\n",
    "\n",
    "model6 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=25,\n",
    "    maxIter=15\n",
    "    )\n",
    "\n",
    "model7 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=30,\n",
    "    maxIter=15\n",
    "    )\n",
    "\n",
    "model8 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=40,\n",
    "    maxIter=15\n",
    "    )\n",
    "\n",
    "model9 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=55,\n",
    "    maxIter=15\n",
    "    )\n",
    "\n",
    "model10 = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=.10,\n",
    "    rank=65,\n",
    "    maxIter=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit models\n",
    "recommender0 = model0.fit(sp_train)\n",
    "recommender1 = model1.fit(sp_train)\n",
    "recommender2 = model2.fit(sp_train)\n",
    "recommender3 = model3.fit(sp_train)\n",
    "recommender4 = model4.fit(sp_train)\n",
    "recommender5 = model5.fit(sp_train)\n",
    "recommender6 = model6.fit(sp_train)\n",
    "recommender7 = model7.fit(sp_train)\n",
    "recommender8 = model8.fit(sp_train)\n",
    "recommender9 = model9.fit(sp_train)\n",
    "recommender10 = model10.fit(sp_train)\n",
    "\n",
    "models = [recommender0, recommender1, recommender2, recommender3, recommender4, \n",
    "          recommender5, recommender6, recommender7, recommender8, recommender9, recommender10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_models(trained_models, testdf, metric='rmse'):\n",
    "    '''\n",
    "    INPUT: list of trained models, and spark dataframe of training data\n",
    "    OUTPUT: list of rmses, and list of ranks associated with models\n",
    "    \n",
    "    trained_models = [recommender1, recommender2]\n",
    "    testdf = spark.createDataFrame(test_pandas_df)\n",
    "    \n",
    "    rmses, ranks =  evaluate_models(trained_models, testdf)\n",
    "    '''\n",
    "    ranks = []\n",
    "    rmses = []\n",
    "    for model in trained_models:\n",
    "        predictions = model.transform(testdf)\n",
    "        pred_df = predictions.toPandas()\n",
    "        rawPredictions = spark.createDataFrame(pred_df.dropna(axis=0))\n",
    "        \n",
    "        predictions = rawPredictions\\\n",
    "        .withColumn(\"rating\", rawPredictions.rating.cast(\"double\"))\\\n",
    "        .withColumn(\"prediction\", rawPredictions.prediction.cast(\"double\"))\n",
    "       \n",
    "        evaluator =\\\n",
    "        RegressionEvaluator(metricName=metric, labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        \n",
    "        ranks.append(model.rank)\n",
    "        rmses.append(rmse)\n",
    "    return rmses, ranks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmses, ranks = evaluate_models(models, sp_test, metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsestrain, rankstrain = evaluate_models(models, sp_train, metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsestrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use 20-25 latent features (rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,7))\n",
    "\n",
    "ax.plot(ranks, rmses, label='Test', c='darkred')\n",
    "ax.plot(rankstrain, rmsestrain, label='Train', c='indianred', ls='dashed')\n",
    "ax.axhline(mean_rmse, label='Mean', ls='dashed', c='dimgray')\n",
    "#ax.set_facecolor('red')\n",
    "\n",
    "ax.set_title(\"Latent Features to RMSE\", fontsize=25)\n",
    "ax.set_ylabel(\"Root Mean Squared Error\", fontsize=15)\n",
    "ax.set_xlabel(\"# Latent Features\", fontsize=15)\n",
    "ax.legend(fontsize=15)\n",
    "#fig.savefig('/home/ubuntu/PROJECT/github-collaborator/matplots/RMSE2.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,7))\n",
    "\n",
    "ax.plot(ranks, rmses, label='Test', c='darkred')\n",
    "ax.plot(rankstrain, rmsestrain, label='Train', c='indianred', ls='dashed')\n",
    "ax.axhline(mean_rmse, label='Mean', ls='dashed', c='dimgray')\n",
    "#ax.set_facecolor('red')\n",
    "\n",
    "ax.set_title(\"Latent Features to RMSE\", fontsize=25)\n",
    "ax.set_ylabel(\"Root Mean Squared Error\", fontsize=15)\n",
    "ax.set_xlabel(\"# Latent Features\", fontsize=15)\n",
    "ax.legend(fontsize=15)\n",
    "fig.savefig('/home/ubuntu/PROJECT/github-collaborator/matplots/RMSE2.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### playing with imputing random 'predictions' instead of actual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.choice([2.0, 4.0, 6.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakedf = pd.DataFrame({'actual': [2.0, 2.0, 6.0, 4.0]})\n",
    "fakedf['prediciton'] = pd.Series([np.random.choice([2.0, 4.0, 6.0]) for _ in range(fakedf.shape[0])])\n",
    "fakedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakemean = np.mean(fakedf.actual)\n",
    "fakemean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakedf['prediciton'] = fakemean\n",
    "fakedf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_als = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=0.1,\n",
    "    rank=50,\n",
    "    maxIter=15\n",
    "    ) \n",
    "final_model = final_als.fit(sp_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_rmses, fin_ranks = evaluate_models([final_model], sp_test, metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"Final model has RMSE of : {:.4f}\".format(fin_rmses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versus predicting random rank for all (2, 4, or 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Using random choice of 2, 4, or 6\n",
    "predictions_random = alsModel.transform(sp_test)\n",
    "\n",
    "pred_df = predictions_random.toPandas()\n",
    "pred_df = pred_df.dropna(axis=0)\n",
    "pred_df['prediction'] = pd.Series([np.random.choice([2.0, 4.0, 6.0]) for _ in range(pred_df.shape[0])])\n",
    "\n",
    "rawPredictions = spark.createDataFrame(pred_df.dropna(axis=0))\n",
    "\n",
    "predictions = rawPredictions\\\n",
    ".withColumn(\"rating\", rawPredictions.rating.cast(\"double\"))\\\n",
    ".withColumn(\"prediction\", rawPredictions.prediction.cast(\"double\"))\n",
    "\n",
    "evaluator =\\\n",
    "RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rand_rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print \"Model imputed with random rank values for predictions has RMSE of : {:.4f}\".format(rand_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model imputed with random rank values for predictions has RMSE of : 0.6152\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Using mean rank\n",
    "predictions_random = recommender10.transform(sp_test)\n",
    "\n",
    "pred_df = predictions_random.toPandas()\n",
    "mean_rank = np.mean(pred_df.rating)\n",
    "pred_df = pred_df.dropna(axis=0)\n",
    "pred_df['prediction'] = mean_rank\n",
    "\n",
    "rawPredictions = spark.createDataFrame(pred_df.dropna(axis=0))\n",
    "\n",
    "predictions = rawPredictions\\\n",
    ".withColumn(\"rating\", rawPredictions.rating.cast(\"double\"))\\\n",
    ".withColumn(\"prediction\", rawPredictions.prediction.cast(\"double\"))\n",
    "\n",
    "evaluator =\\\n",
    "RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mean_rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print \"Model imputed with random rank values for predictions has RMSE of : {:.4f}\".format(mean_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.save(\"/home/ubuntu/PROJECT/github-collaborator/data/models/finalModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "als = ALS(nonnegative=True)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.maxIter, [15, 30]) \\\n",
    "    .addGrid(als.regParam, [1, 0.1, 0.01, 5]) \\\n",
    "    .addGrid(als.rank, [10,15,20,25,30,35,40]) \\\n",
    "    .build()\n",
    "    \n",
    "crossval = CrossValidator(estimator=als,\n",
    "                      estimatorParamMaps=paramGrid,\n",
    "                      evaluator=RegressionEvaluator(\n",
    "                          metricName=\"rmse\", \n",
    "                          labelCol=\"rating\"),\n",
    "                      numFolds=7)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "alsModel = crossval.fit(sp_train)\n",
    "alsModel.bestModel.save('/home/ubuntu/PROJECT/github-collaborator/data/models/CVmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alsModel = ALSModel.load('/home/ubuntu/PROJECT/github-collaborator/data/models/CVmodel')\n",
    "alsModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvrmses, cvranks = evaluate_models([alsModel.bestModel], sp_test, metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvrmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using log of number of commits per repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>log10_commits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>51746271</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>56271530</td>\n",
       "      <td>1.176091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>63915386</td>\n",
       "      <td>1.146128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.681241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   repo_id  log10_commits\n",
       "0        1         1       0.954243\n",
       "1        1  51746271       1.000000\n",
       "2        1  56271530       1.176091\n",
       "3        1  63915386       1.146128\n",
       "4        2         1       1.681241"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entirecommdata = pd.read_csv('data/new_subset_data/entire_commits.csv')\n",
    "entirecommdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_entire_commits = spark.createDataFrame(entirecommdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+----+--------+------------------+\n",
      "|user|    item|            rating|\n",
      "+----+--------+------------------+\n",
      "|   1|       1|    0.954242509439|\n",
      "|   1|51746271|               1.0|\n",
      "|   1|56271530|1.1760912590600001|\n",
      "|   1|63915386|1.1461280356799999|\n",
      "|   2|       1|     1.68124123738|\n",
      "|   2|       2|1.7634279935599997|\n",
      "|   2|   25532|     1.94939000664|\n",
      "|   2|   28923|3.3398487830400003|\n",
      "|   2|   49907|2.8864907251700003|\n",
      "|   2|   76547|     1.27875360095|\n",
      "|   2|  133800|     1.86332286012|\n",
      "|   2|  169210|     2.18469143082|\n",
      "|   2|  187332|1.5910646070299999|\n",
      "|   2|  258837|     2.12710479836|\n",
      "|   2|  398246|1.5440680443500001|\n",
      "|   2|  504851|2.0453229787900002|\n",
      "|   2|  525267|2.3344537511500003|\n",
      "|   2|  623841|1.5682017240700001|\n",
      "|   2|  638698|2.0334237554900003|\n",
      "|   2|  650825|     1.90308998699|\n",
      "+----+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change column names to something Spark has hardcoded into their GridSearch feature for dataframes\n",
    "oldColumns = sp_entire_commits.schema.names\n",
    "newColumns = [\"user\", \"item\", \"rating\"]\n",
    "\n",
    "sp_entire_commits = reduce(lambda sp_entire_commits, idx: sp_entire_commits.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), sp_entire_commits)\n",
    "sp_entire_commits.printSchema()\n",
    "sp_entire_commits.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_train_commits, sp_test_commits, sp_val_commits = sp_entire_commits.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commits_als = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=10.0,\n",
    "    rank=55,\n",
    "    maxIter=20,\n",
    "    implicitPrefs=True,\n",
    "    alpha=40.0\n",
    "    ) \n",
    "commits_als = commits_als.fit(sp_train_commits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_commits_als = ALS(\n",
    "    itemCol='item',\n",
    "    userCol='user',\n",
    "    ratingCol='rating',\n",
    "    nonnegative=True,    \n",
    "    regParam=10.0,\n",
    "    rank=55,\n",
    "    maxIter=20,\n",
    "    implicitPrefs=True,\n",
    "    alpha=40.0\n",
    "    ) \n",
    "small_commits_als = commits_als.fit(sp_test_commits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59366945 entries, 0 to 59366944\n",
      "Data columns (total 6 columns):\n",
      "user_id        int64\n",
      "login          object\n",
      "repo_id        int64\n",
      "forked_from    object\n",
      "repo_name      object\n",
      "url            object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 2.7+ GB\n"
     ]
    }
   ],
   "source": [
    "users_repos = pd.read_csv('data/user_repo_lookup.csv', sep='\\t')\n",
    "users_repos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7623079    3384558\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(users_repos['user_id'].loc[users_repos['login'] == 'caitriggs'])[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_repos(model, lookupdf, username):\n",
    "    user_id = int((lookupdf['user_id'].loc[lookupdf['login'] == username])[:1])\n",
    "    prediction_df = pd.DataFrame(lookupdf['repo_id'])\n",
    "    prediction_df['user'] = user_id\n",
    "    prediction_df = prediction_df.rename(columns={'id':'item'})\n",
    "    \n",
    "    pred_df = spark.createDataFrame(prediction_df)\n",
    "    predictions_user = model.transform(pred_df)\n",
    "    return predictions_user.sort(desc(\"prediction\")).show(10)\n",
    "\n",
    "#toPandas().sort_values('prediction', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "| 10|[0.0, 0.0, 0.0, 0...|\n",
      "| 20|[0.0, 0.0, 0.0, 0...|\n",
      "| 30|[0.0, 0.0, 0.0, 0...|\n",
      "| 40|[1.292633E-4, 0.0...|\n",
      "| 80|[0.0, 0.0, 0.0, 0...|\n",
      "| 90|[0.0, 0.0, 0.0, 0...|\n",
      "|100|[0.03258656, 0.0,...|\n",
      "|130|[0.0, 0.0, 0.0, 0...|\n",
      "|160|[0.0, 0.0, 0.0, 0...|\n",
      "|170|[0.0, 0.0, 0.0, 0...|\n",
      "|190|[0.0, 0.002047706...|\n",
      "|230|[0.0, 0.0, 0.0, 0...|\n",
      "|240|[0.0, 0.0, 0.0, 0...|\n",
      "|250|[0.0, 0.0, 1.9977...|\n",
      "|280|[0.0, 0.0, 0.0, 0...|\n",
      "|310|[0.0, 0.0, 0.0, 0...|\n",
      "|340|[0.0, 0.0, 0.0, 0...|\n",
      "|350|[0.0, 0.0, 0.0, 0...|\n",
      "|370|[0.0, 0.0, 0.0, 1...|\n",
      "|380|[0.0, 0.0, 0.0, 0...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commits_als.itemFactors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ecb1b924d759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcait_reco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommend_repos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommits_als\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers_repos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'caitriggs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcait_reco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-41509bf3ef11>\u001b[0m in \u001b[0;36mrecommend_repos\u001b[0;34m(model, lookupdf, username)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprediction_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpredictions_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions_user\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    320\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    321\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fields)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0;34m\"fields should be a list of StructField\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_needSerializeAnyField\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneedConversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnullable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cait_reco = recommend_repos(commits_als, users_repos, 'caitriggs')\n",
    "cait_reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gavin_reco = recommend_repos(commits_als, users_repos, 'gavin-peterkin')\n",
    "gavin_reco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.save(\"/home/ubuntu/PROJECT/github-collaborator/data/models/finalModel_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model = ALSModel.load(\"/home/ubuntu/PROJECT/github-collaborator/data/models/finalModel_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commitsModel = ALSModel.load(\"/home/ubuntu/PROJECT/github-collaborator/data/models/commitsModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_repos = pd.read_csv('data/user_repo_lookup.csv', sep='\\t')\n",
    "users_repos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int(users['id'].loc[users['login'] == 'caitriggs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend_repos(model, lookupdf, username):\n",
    "    user_id = int((lookupdf['user_id'].loc[lookupdf['login'] == username])[:1])\n",
    "    prediction_df = pd.DataFrame(lookupdf['repo_id'])\n",
    "    prediction_df['user'] = user_id\n",
    "    prediction_df = prediction_df.rename(columns={'id':'item'})\n",
    "    \n",
    "    pred_df = spark.createDataFrame(prediction_df)\n",
    "    predictions_user = model.transform(pred_df)\n",
    "    return predictions_user.sort(desc(\"prediction\")).show(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cait_reco = recommend_repos(Model, users_repos, 'caitriggs')\n",
    "cait_reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Top recommendations from Model\n",
    "users_repos[users_repos.repo_id == 2559614]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top recommendations from CVModel\n",
    "repos[repos.id == 25260835]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
